{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+------------------+----------------------+-----------------------+--------------+-----------+-------------------+----+-----+---+----------+----------+------------+------------+\n",
      "|SENDER_SITE_LOC_ID|RECEIVER_SITE_LOC_ID|ITEMS_TOTAL_AMOUNT|INVOICED_ENTITY_LOC_ID|INVOICING_ENTITY_LOC_ID|INVOICE_NUMBER|GROUPING_ID|CUSTOMS_REGIME_CODE|year|month|day|LAT_SENDER|LON_SENDER|LAT_RECEIVER|LON_RECEIVER|\n",
      "+------------------+--------------------+------------------+----------------------+-----------------------+--------------+-----------+-------------------+----+-----+---+----------+----------+------------+------------+\n",
      "|             15793|               16021|      200541749408|                 25823|                  16222|       8355417|     137861|               1000|2017|    5|  2|     44.34|     26.03|       48.83|        2.35|\n",
      "|             15793|               16021|       28521692304|                 25823|                  16222|       8380266|     140173|               1000|2017|    7| 18|     44.34|     26.03|       48.83|        2.35|\n",
      "|             15793|               16021|       34084668024|                 25823|                  16222|       8356202|     137006|               4000|2017|    5|  3|     44.34|     26.03|       48.83|        2.35|\n",
      "|             15793|               16021|        5292972000|                 25823|                  16222|       8355463|     136799|               4000|2017|    5|  2|     44.34|     26.03|       48.83|        2.35|\n",
      "|             15705|                8415|         529618987|                 24912|                  16222|       8385160|     140265|               1000|2017|    8|  3|     59.88|     10.13|       33.23|       -5.61|\n",
      "|             15695|               16021|             87400|                 25823|                  16222|       8383523|     140271|               4000|2017|    7| 28|     40.33|     -3.68|       48.83|        2.35|\n",
      "|             15793|               50899|          11336936|                 16222|             1298112186|         58056|     393043|               1000|2017|    7| 11|     44.34|     26.03|       41.88|       12.48|\n",
      "|             15695|               16021|            145860|                 25823|                  16222|       8380336|     140161|               4000|2017|    7| 18|     40.33|     -3.68|       48.83|        2.35|\n",
      "|             15793|               16021|        5632544000|                 25823|                  16222|       8380252|     140150|               4000|2017|    7| 18|     44.34|     26.03|       48.83|        2.35|\n",
      "|             15695|               16021|             75600|                 25823|                  16222|       8380335|     140158|               1000|2017|    7| 18|     40.33|     -3.68|       48.83|        2.35|\n",
      "|             15793|               16021|        5632544000|                 25823|                  16222|       8380367|     140167|               4000|2017|    7| 18|     44.34|     26.03|       48.83|        2.35|\n",
      "|             15695|               16021|            105000|                 25823|                  16222|       8380334|     140157|               4000|2017|    7| 18|     40.33|     -3.68|       48.83|        2.35|\n",
      "|             15793|               16021|         619489728|                 25823|                  16222|       8355417|     137861|               2100|2017|    5|  2|     44.34|     26.03|       48.83|        2.35|\n",
      "|             15705|                8415|         529618987|                 24912|                  16222|       8385160|     140266|               4000|2017|    8|  3|     59.88|     10.13|       33.23|       -5.61|\n",
      "|             15793|               16021|       13641992760|                 25823|                  16222|       8355469|     136805|               4000|2017|    5|  2|     44.34|     26.03|       48.83|        2.35|\n",
      "|             15695|               15572|             25992|                 24580|                  16222|       8381004|     140168|               1000|2017|    7| 19|     40.33|     -3.68|        39.9|       34.73|\n",
      "|             15695|               16021|            163300|                 25823|                  16222|       8380333|     140154|               1000|2017|    7| 18|     40.33|     -3.68|       48.83|        2.35|\n",
      "|             15695|               16021|             75600|                 25823|                  16222|       8380332|     140152|               1000|2017|    7| 18|     40.33|     -3.68|       48.83|        2.35|\n",
      "|             15793|               16021|        5292972000|                 25823|                  16222|       8355465|     136801|               4000|2017|    5|  2|     44.34|     26.03|       48.83|        2.35|\n",
      "|              7597|               15710|         851859481|                 17611|                  16222|     282965018|     461615|               4000|2018|    1|  8|     55.75|     37.63|      51.446|       -0.18|\n",
      "+------------------+--------------------+------------------+----------------------+-----------------------+--------------+-----------+-------------------+----+-----+---+----------+----------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import pyspark\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "### function to transform date\n",
    "\n",
    "#date1 = \"31/12/2015\"\n",
    "#newdate1 = time.strptime(date1, \"%d/%m/%Y\")\n",
    "\n",
    "\n",
    "# The date has to be with the form DD/MM/YYYY and our data scheme is DD/MM/YY\n",
    "# this function adds 20 to put the date to the correct form. We can do that because all of our dates are from the same century\n",
    "\n",
    "def transform_date(string, index):\n",
    "    return string[:index] + '20' + string[index:]\n",
    "\n",
    "### This is the structure to access the date data\n",
    "\n",
    "#print(newdate1.tm_year)\n",
    "#print(newdate1.tm_mon)\n",
    "#print(newdate1.tm_mday)\n",
    "\n",
    "#print(transform_date(date1, 6))\n",
    "\n",
    "\n",
    "# Set up the spark session\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "   .master(\"spark://spark-master:7077\") \\\n",
    "   .appName(\"romain2\") \\\n",
    "   .config(conf = pyspark.SparkConf()) \\\n",
    "   .getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "# Read the csv File\n",
    "\n",
    "df1 = spark.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('../Bureau/SOD/TBF110_DECLARATION_DATA_TABLE.csv')\n",
    "\n",
    "\n",
    "# Delete the frist line. I did it beacuse there were two date missing from the file. And it is juste a proof of concept\n",
    "# In production date field is filled\n",
    "\n",
    "dftemp1 = df1.dropDuplicates()\n",
    "dfReadyToTransform = dftemp1.dropDuplicates()\n",
    "\n",
    "\n",
    "\n",
    "# We transform date from our dataset\n",
    "\n",
    "rdd = dfReadyToTransform.rdd.map(lambda p: transform_date(p.INVOICE_DATE, 6))\n",
    "\n",
    "\n",
    "# Three columns are created to display year, months and days\n",
    "# datedf returns a dataframe\n",
    "\n",
    "datedf = rdd.map(lambda x: (x,time.strptime(x, \"%d/%m/%Y\").tm_year,time.strptime(x, \"%d/%m/%Y\").tm_mon,time.strptime(x, \"%d/%m/%Y\").tm_mday)).toDF()\n",
    "#datedf.show()\n",
    "\n",
    "\n",
    "# Rename the df columns\n",
    "\n",
    "datedf = datedf.withColumnRenamed('_2', 'year')\n",
    "datedf = datedf.withColumnRenamed('_3', 'month')\n",
    "datedf = datedf.withColumnRenamed('_4', 'day')\n",
    "#datedf.show()\n",
    "\n",
    "# Read the csv file and transforms it to dataframe\n",
    "\n",
    "df2 = spark.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('../Bureau/SOD/TBF134_CUSTOMS_FILE_GROUP_DATA_TABLE.csv')\n",
    "\n",
    "\n",
    "# Create an id to the two dataframe to add easily datedf columns. It is a tips to add column to a dataframe\n",
    "\n",
    "datedf = datedf.withColumn(\"rowId\", monotonically_increasing_id())\n",
    "dfReady = dfReadyToTransform.withColumn(\"rowId\", monotonically_increasing_id())\n",
    "#datedf.show()\n",
    "\n",
    "\n",
    "# Select all data we are interested to study from the two dataframes\n",
    "\n",
    "finaldf = dfReady.join(datedf, 'rowId', 'inner').select('SENDER_SITE_LOC_ID','RECEIVER_SITE_LOC_ID','ITEMS_TOTAL_AMOUNT','INVOICED_ENTITY_LOC_ID','INVOICING_ENTITY_LOC_ID','INVOICE_NUMBER','GROUPING_ID','CUSTOMS_REGIME_CODE', 'year', 'month', 'day','LAT_SENDER','LON_SENDER','LAT_RECEIVER','LON_RECEIVER')\n",
    "finaldf.show()\n",
    "\n",
    "\n",
    "finaldf.toPandas().to_csv('romain2.csv')\n",
    "\n",
    "\n",
    "#finaldf.write.csv('romain.csv')\n",
    "\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
