{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'findspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0c48b9766d4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mholoviews\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'findspark'"
     ]
    }
   ],
   "source": [
    "!pip install findspark\n",
    "import findspark\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import holoviews as hv\n",
    "import networkx as nx\n",
    "import geoviews as gv\n",
    "\n",
    "\n",
    "\n",
    "# Initialize and provide path\n",
    "findspark.init(\"/usr/local/spark\")\n",
    "\n",
    "\n",
    "from cartopy import crs as ccrs\n",
    "\n",
    "import geoviews.tile_sources as gts\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "# Set up the spark session\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "   .master(\"local\") \\\n",
    "   .appName(\"viz\") \\\n",
    "   .config(\"spark.executor.memory\", \"3gb\") \\\n",
    "   .getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "# Read the csv File\n",
    "\n",
    "df1 = spark.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('romain.csv')\n",
    "\n",
    "\n",
    "# dfsender is a dataframe with single couple (id,latitude,longitude) for the sender\n",
    "dfsender = df1.dropDuplicates(['SENDER_SITE_LOC_ID', 'LAT_SENDER', 'LON_SENDER'])\n",
    "\n",
    "# dfreceiver is a dataframe with single couple (id,latitude,longitude) for the receiver\n",
    "dfreceiver = df1.dropDuplicates(['RECEIVER_SITE_LOC_ID', 'LAT_RECEIVER', 'LON_RECEIVER'])\n",
    "\n",
    "\n",
    "# Select only the useful parameters\n",
    "dfsender = dfsender.select(['SENDER_SITE_LOC_ID', 'LAT_SENDER', 'LON_SENDER'])\n",
    "dfreceiver = dfreceiver.select(['RECEIVER_SITE_LOC_ID', 'LAT_RECEIVER', 'LON_RECEIVER'])\n",
    "\n",
    "\n",
    "# Rename the parameters to merge\n",
    "dfsender = dfsender.select(col(\"SENDER_SITE_LOC_ID\").alias(\"id\"), col(\"LAT_SENDER\").alias(\"latitude\"), col(\"LON_SENDER\").alias(\"longitude\"))\n",
    "dfreceiver = dfreceiver.select(col(\"RECEIVER_SITE_LOC_ID\").alias(\"id\"), col(\"LAT_RECEIVER\").alias(\"latitude\"), col(\"LON_RECEIVER\").alias(\"longitude\"))\n",
    "\n",
    "\n",
    "# df_concat is ready to be used, it is formatted and takes few element, pandas datafram can be called\n",
    "df_concat = dfsender.union(dfreceiver)\n",
    "df_concat.show()\n",
    "\n",
    "\n",
    "\n",
    "# transform to pandas, can be done if the data are not too big. That is why the previous treatment id done in pyspark\n",
    "dfpandas = df_concat.toPandas()\n",
    "dataframe = df1.dropDuplicates(['SENDER_SITE_LOC_ID', 'RECEIVER_SITE_LOC_ID']).toPandas()\n",
    "#dataframe = df1.toPandas()\n",
    "\n",
    "dfpandas.to_csv('coords.csv')\n",
    "dataframe.to_csv('links.csv')\n",
    "\n",
    "print(dfpandas)\n",
    "print(dataframe)\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
